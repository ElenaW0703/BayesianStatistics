---
title: "Lab9_STA602"
author: "ElenaW."
date: "11/15/2021"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r}
require(tidyverse)
require(rstanarm)
require(magrittr)
library(ggplot2)
library(mlmRev)
library(tidybayes)
library(ggstance)
library(dplyr)
library(modelr)
library(brms)
```

```{r}
data(Gcsemv, package = "mlmRev")
dim(Gcsemv)
```

```{r}
summary(Gcsemv)
```

```{r}
# Make Male the reference category and rename variable
Gcsemv$female <- relevel(Gcsemv$gender, "M")

# Use only total score on coursework paper
GCSE <- subset(x = Gcsemv,
               select = c(school, student, female, course))

# Count unique schools and students
m <- length(unique(GCSE$school))
N <- nrow(GCSE)
```

## Exercise 1

```{r}
GCSE = na.omit(GCSE)
average_course = GCSE %>%
  group_by(school) %>%
  summarise(average_course = mean(course))
average_course
```

```{r}
hist(average_course$average_course,
     xlab = "Average Course Scores",
     main = "Histogram of Average Course Scores")
```

From the histogram above, we could see that the distribution is left skewed, and the overall average course scores of schools kind of spread off. Students in different schools performs differently. Thus, may it's not a good idea to do information share for mean in the hierarchical model. 

```{r}
pooled <- stan_glm(course ~ 1 + female, data = GCSE, refresh = 0)
unpooled <- stan_glm(course ~ -1 + school + female,data=GCSE, refresh = 0)
```

```{r}
mod1 <- stan_lmer(formula = course ~ 1 + (1 | school),
                  data = GCSE,
                  seed = 349,
                  refresh = 0)
```

```{r}
prior_summary(object = mod1)
```

```{r}
sd(GCSE$course, na.rm = T)
```
```{r}
print(mod1, digits = 3)
```

```{r}
summary(mod1,
        pars = c("(Intercept)", "sigma", "Sigma[school:(Intercept),(Intercept)]"),
        probs = c(0.025, 0.975),
        digits = 3)
```

### Exercise 2

From the code above, µθ is 73.665, σ is 13.819
τ^2 is 78.667. 

```{r}
mod1_sims <- as.matrix(mod1)
dim(mod1_sims)
```

```{r}
par_names <- colnames(mod1_sims)
head(par_names)
```

```{r}
tail(par_names)
```

```{r}
# obtain draws for mu_theta
mu_theta_sims <- as.matrix(mod1, pars = "(Intercept)")

# obtain draws for each school's contribution to intercept
omega_sim <- as.matrix(mod1,
                        regex_pars ="b\\[\\(Intercept\\) school\\:")

# to finish: obtain draws for sigma and tau^2
sig_sims <- as.matrix(mod1,
                      pars = "sigma")
tau2_sims <- as.matrix(mod1,
                       pars = "Sigma[school:(Intercept),(Intercept)]")
```

```{r}
# posterior samples of intercepts, which is overall intercept + school-specific intercepts
int_sims <- as.numeric(mu_theta_sims) + omega_sim

# posterior mean
int_mean <- apply(int_sims, MARGIN = 2, FUN = mean)

# credible interval
int_ci <- apply(int_sims, MARGIN = 2, FUN = quantile, probs = c(0.025, 0.975))
int_ci <- data.frame(t(int_ci))

# combine into a single df
int_df <- data.frame(int_mean, int_ci)
names(int_df) <- c("post_mean","Q2.5", "Q97.5")

# sort DF according to posterior mean
int_df <- int_df[order(int_df$post_mean),]

# create variable "index" to represent order
int_df <- int_df %>% mutate(index = row_number())

# plot posterior means of school-varying intercepts, along with 95 CIs
ggplot(data = int_df, aes(x = index, y = post_mean))+
  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5))+
  scale_x_continuous("Index", breaks = seq(0,m, 5)) +
  scale_y_continuous(expression(paste("varying intercept ", theta[j], " = ", mu[theta]+omega[j])))
```

### Exercise 3

```{r}
school_20920 = as.matrix(mod1, pars = "b[(Intercept) school:20920]")
school_22520 = as.matrix(mod1, pars = "b[(Intercept) school:22520]")
diff = school_20920 - school_22520
summary(diff)
```

```{r}
hist(diff)
```

From the results above, we could see that the difference between school 20920 and school 22520 is from -10 to 23.7, and the center is around 6.5, which means that the posterior averages of this two school are different and we don't have strong evidence say that which one is higher than another. 

## Model 2: Varying intercept with a single individual-level predictor 

```{r}
mod2 <- stan_lmer(formula = course ~ 1 + female + (1 | school),
                  data = GCSE, 
                  prior = normal(location = 0,
                                        scale = 100,
                                        autoscale = F),
                  prior_intercept = normal(location = 0,
                                        scale = 100,
                                        autoscale = F),
                  seed = 349,
                  refresh = 0)
```

```{r}
# plot varying intercepts
mod2.sims <- as.matrix(mod2)
group_int <- mean(mod2.sims[,1])
mp <- mean(mod2.sims[,2])
bp <- apply(mod2.sims[, 3:75], 2, mean)
xvals <- seq(0,1,.01)
plot(x = xvals, y = rep(0, length(xvals)), 
     ylim = c(50, 90), xlim = c(-0.1,1.1), xaxt = "n", xlab = "female", ylab = "course")
axis(side = 1, at = c(0,1))
for (bi in bp){
  lines(xvals, (group_int + bi)+xvals*mp)
}
```

### Exercise 4

```{r}
summary(mod2,
pars = c("(Intercept)", "sigma", "femaleF","Sigma[school:(Intercept),(Intercept)]"),
probs = c(0.025, 0.975),
digits = 3)
```

From the results above, we could see that µθ is 69.661, σ is 13.427, τ^2 is 80.900 and β is 6.743. 

## Model 3

```{r}
mod3 <- stan_lmer(formula = course~ 1+ female + (1 + female | school),
                  data = GCSE,
                  seed = 349,
                  refresh = 0)
mod3_sims <- as.matrix(mod3)

# obtain draws for mu_theta
mu_theta_sims <- as.matrix(mod3, pars = "(Intercept)")

fem_sims <- as.matrix(mod3, pars = "femaleF")
# obtain draws for each school's contribution to intercept
omega_sims <- as.matrix(mod3,
                        regex_pars ="b\\[\\(Intercept\\) school\\:")
beta_sims <- as.matrix(mod3,
                       regex_pars ="b\\[femaleF school\\:")

int_sims <- as.numeric(mu_theta_sims) + omega_sims
slope_sims <- as.numeric(fem_sims) + beta_sims

# posterior mean
slope_mean <- apply(slope_sims, MARGIN = 2, FUN = mean)

# credible interval
slope_ci <- apply(slope_sims, MARGIN = 2, FUN = quantile, probs = c(0.025, 0.975))
slope_ci <- data.frame(t(slope_ci))

# combine into a single df
slope_df <- data.frame(slope_mean, slope_ci, levels(GCSE$school))
names(slope_df) <- c("post_mean","Q2.5", "Q97.5", "school")

# sort DF according to posterior mean
slope_df <- slope_df[order(slope_df$post_mean),]

# create variable "index" to represent order
slope_df <- slope_df %>% mutate(index = row_number())

# plot posterior means of school-varying slopes, along with 95% CIs
ggplot(data = slope_df, aes(x = index, y = post_mean))+
  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5))+
  scale_x_continuous("Index", breaks = seq(1,m, 1),
                     labels = slope_df$school) +
  scale_y_continuous(expression(paste("varying slopes ", beta[j])))+
  theme(axis.text.x = element_text(angle = 90))
```

```{r}
loo1 <- loo(mod1)
loo2 <- loo(mod2)
loo3 <- loo(mod3)
loo_compare(loo1,loo2,loo3)

##      elpd_diff se_diff
## mod3   0.0       0.0  
## mod2 -29.6       9.9  
## mod1 -79.4      15.1
loo_compare(loo1, loo3)
##      elpd_diff se_diff
## mod3   0.0       0.0  
## mod1 -79.4      15.1
```

```{r}
pooled.sim <- as.matrix(pooled)
unpooled.sim <- as.matrix(unpooled)
m1.sim <- as.matrix(mod1)
m2.sim <- as.matrix(mod2)
m3.sim <- as.matrix(mod3)
schools <- unique(GCSE$school)


alpha2 = mean(m2.sim[,1])
alpha3 <- mean(m3.sim[,1])

partial.fem2 <- mean(m2.sim[,2])
partial.fem3 <- mean(m3.sim[,2])
unpooled.fem <- mean(unpooled.sim[,74])

par(mfrow = c(2, 3), mar = c(1,2,2,1))
for (i in 1:18){
  temp = GCSE %>% filter(school == schools[i]) %>%
    na.omit()
  y <- temp$course
  x <- as.numeric(temp$female)-1
  plot(x + rnorm(length(x)) *0.001, y, ylim = c(35,101), xlab = "female",main =schools[i], xaxt = "n", ylab = "course")
  axis(1,c(0,1),cex.axis=0.8)
  
  # no pooling
  b = mean(unpooled.sim[,i])

  # plot lines and data
  xvals = seq(-0.1, 1.1, 0.01)
  lines(xvals, xvals * mean(pooled.sim[,2]) + mean(pooled.sim[,1]), col = "red") # pooled
  lines(xvals, xvals * unpooled.fem + b, col = "blue") # unpooled
  lines(xvals, xvals*partial.fem2 + (alpha2 + mean(m2.sim[,i+2])) , col = "green") # varying int
  lines(xvals, xvals*(partial.fem3 + mean(m3.sim[, 2 + i*2])) + (alpha3 + mean(m3.sim[, 1 + i*2])), col = "orange") # varying int and slope
  legend("bottom", legend = paste("n =", length(y), " "))
}
```

### Exercise 5

From the regression lines above, we could see that the model 3 perform better than model 1 and 2 since the model 3 is more flexible. From Bayesian Shrinkage perspective, the green line (model2) is between red and blue lines. If the sample size is large, green would be closer to the blue line, and if the sample size is smaller, it would be closer to the red line, which means that model 2 will more depend on the sample size of schools. 
From loo_compare result, we could see that model1 and model2 have negative value compared to model 3, which means that the model 3 perform better. 

```{r}
radon <- read.csv("radon.txt", header = T,sep="")
radon$county <- as.factor(radon$county)
```

```{r}
average_radon = radon %>%
  group_by(county) %>%
  summarise(average_radon = mean(log_radon,na.rm = T))
average_radon
```

```{r}
hist(average_radon$average_radon,
     xlab = "Average Radon Level",
     main = "Histogram of Average Radon Level")
```

From the histogram above, we could see that the average of log_radon is different across the the counties, it's good idea to do the hierarchical model but the information may not be shared. 

### Exercise 7

```{r}
radon.unpooled <- stan_glm(log_radon ~ -1 + county,data=radon, refresh = 0)
```

```{r}
radon.mod1 <- stan_lmer(formula = log_radon ~ 1 + (1 | county),
data = radon,
seed = 349,
refresh = 0)
n_county <- as.numeric(table(radon$county))
create_df <- function(sim,model){
mean <- apply(sim,2,mean)
sd <- apply(sim,2,sd)
df <- cbind(n_county, mean, sd) %>%
as.data.frame()%>%
mutate(se = sd/ sqrt(n_county), model = model)
return(df)
}
unpooled.sim <- as.matrix(radon.unpooled)
unpooled.df <- create_df(unpooled.sim[,1:85], model = "unpooled")

mod1.sim <- as.matrix(radon.mod1)[,1:86]
mod1.sim <- (mod1.sim[,1] + mod1.sim)[,-1]
partial.df <- create_df(mod1.sim, model = "partial")
ggplot(rbind(unpooled.df, partial.df)%>% mutate(model = factor(model, levels = c("unpooled", "partial"))), aes(x= n_county, y = mean)) +
#draws the means
geom_jitter() +
#draws the CI error bars
geom_errorbar(aes(ymin=mean-2*se, ymax= mean+2*se), width=.1)+
ylim(0,3)+
xlim(0,60)+
geom_hline(aes(yintercept= mean(coef(radon.unpooled))))+
facet_wrap(~model)
```

From the plots above, we could see that the Bayesian Shrinkage would be weaker with the larger sample size in both plots, and with the larger sample size, there is more uncertainty and larger posterior credible interval for mean. 

### Exercise 8

```{r}
radon.mod2 <- stan_lmer(formula = log_radon ~ 1 + floor + (1 | county),
data = radon,
prior = normal(location = 0,
scale = 100,
autoscale = F),
prior_intercept = normal(location = 0,
scale = 100,
autoscale = F),
seed = 349,
refresh = 0)
radon.mod3 <- stan_lmer(formula = log_radon ~ 1+ floor + (1 + floor | county),
data = radon,
seed = 349,
refresh = 0)
radon.mod4 <- stan_lmer(formula = log_radon ~ 1 + floor + log_uranium + (1 | county),
data = radon,
prior = normal(location = 0,
scale = 100,
autoscale = F),
prior_intercept = normal(location = 0,
scale = 100,
autoscale = F),
seed = 349,
refresh = 0)
```

```{r}
loo1 <- loo(radon.mod1)
loo2 <- loo(radon.mod2)
loo3 <- loo(radon.mod3)
loo4 <- loo(radon.mod4)
loo_compare(loo1,loo2,loo3,loo4)
```

From the result above, we could see that the model 4 has the best performance compared to other negative models. 

### Exercise 9

With larger sample size, the Bayesian shrinkage would be less towards the other groups, which means that there would be less information borrowing/sharing. In other words, information sharing would be better to use for smaller sample size and modeling each group totally separately would be not a good idea in small sample size groups. 

